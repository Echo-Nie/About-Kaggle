{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d66b609",
   "metadata": {
    "papermill": {
     "duration": 0.004655,
     "end_time": "2025-02-04T13:11:49.899303",
     "exception": false,
     "start_time": "2025-02-04T13:11:49.894648",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ℹ️ Info¶\n",
    "\n",
    "* **forked original great work kernels**\n",
    "    * (YOLO) https://www.kaggle.com/code/itsuki9180/czii-yolo11-submission-baseline    \n",
    "    * (YOLO) https://www.kaggle.com/code/sersasj/czii-yolo11-submission-baseline-with-kdtree-update\n",
    "    * (Unet3D) https://www.kaggle.com/code/ahsuna123/3d-u-net-training-only\n",
    "    * (Unet3D) https://www.kaggle.com/code/fnands/baseline-unet-train-submit\n",
    "    * (Unet3D) https://www.kaggle.com/code/linheshen/esemble-2d-and-3d\n",
    "    \n",
    "* **2025/01/15 My Additional**\n",
    "    * Unet3D-Monai local train model add. Model dataset is here.\n",
    "    * https://www.kaggle.com/datasets/hideyukizushi/cziials-a-230-unet/data\n",
    "    ```\n",
    "    # train validation score&loss\n",
    "    val_metric=0.450\n",
    "    train_loss=0.593\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0158a134",
   "metadata": {
    "papermill": {
     "duration": 0.003635,
     "end_time": "2025-02-04T13:11:49.907053",
     "exception": false,
     "start_time": "2025-02-04T13:11:49.903418",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c276abf",
   "metadata": {
    "papermill": {
     "duration": 0.003499,
     "end_time": "2025-02-04T13:11:49.914292",
     "exception": false,
     "start_time": "2025-02-04T13:11:49.910793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **《《《 YOLO 》》》**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bfd7b21",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-04T13:11:49.922641Z",
     "iopub.status.busy": "2025-02-04T13:11:49.922406Z",
     "iopub.status.idle": "2025-02-04T13:13:11.546163Z",
     "shell.execute_reply": "2025-02-04T13:13:11.545471Z"
    },
    "papermill": {
     "duration": 81.629713,
     "end_time": "2025-02-04T13:13:11.547696",
     "exception": false,
     "start_time": "2025-02-04T13:11:49.917983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!tar xfvz /kaggle/input/ultralytics-for-offline-install/archive.tar.gz\n",
    "!pip install --no-index --find-links=./packages ultralytics\n",
    "!rm -rf ./packages\n",
    "try:\n",
    "    import zarr\n",
    "except: \n",
    "    !cp -r '/kaggle/input/hengck-czii-cryo-et-01/wheel_file' '/kaggle/working/'\n",
    "    !pip install /kaggle/working/wheel_file/asciitree-0.3.3/asciitree-0.3.3\n",
    "    !pip install --no-index --find-links=/kaggle/working/wheel_file zarr\n",
    "    !pip install --no-index --find-links=/kaggle/working/wheel_file connected-components-3d\n",
    "from typing import List, Tuple, Union\n",
    "deps_path = '/kaggle/input/czii-cryoet-dependencies'\n",
    "! pip install -q --no-index --find-links {deps_path} --requirement {deps_path}/requirements.txt\n",
    "import lightning.pytorch as pl\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import sys\n",
    "sys.path.append('/kaggle/input/hengck-czii-cryo-et-01')\n",
    "from czii_helper import *\n",
    "from dataset import *\n",
    "from model2 import *\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "293b0b18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T13:13:11.556744Z",
     "iopub.status.busy": "2025-02-04T13:13:11.556511Z",
     "iopub.status.idle": "2025-02-04T13:13:11.568128Z",
     "shell.execute_reply": "2025-02-04T13:13:11.567596Z"
    },
    "papermill": {
     "duration": 0.017433,
     "end_time": "2025-02-04T13:13:11.569382",
     "exception": false,
     "start_time": "2025-02-04T13:13:11.551949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('/kaggle/input/tttttt')\n",
    "from pred import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ed73f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T13:13:11.577685Z",
     "iopub.status.busy": "2025-02-04T13:13:11.577472Z",
     "iopub.status.idle": "2025-02-04T13:13:13.934188Z",
     "shell.execute_reply": "2025-02-04T13:13:13.933524Z"
    },
    "papermill": {
     "duration": 2.362437,
     "end_time": "2025-02-04T13:13:13.935571",
     "exception": false,
     "start_time": "2025-02-04T13:13:11.573134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "import zarr\n",
    "from scipy.spatial import cKDTree\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3035c790",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T13:13:13.944492Z",
     "iopub.status.busy": "2025-02-04T13:13:13.944236Z",
     "iopub.status.idle": "2025-02-04T13:13:15.666686Z",
     "shell.execute_reply": "2025-02-04T13:13:15.665662Z"
    },
    "papermill": {
     "duration": 1.728896,
     "end_time": "2025-02-04T13:13:15.668677",
     "exception": false,
     "start_time": "2025-02-04T13:13:13.939781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = '/kaggle/input/czii-yolo-l-trained-with-synthetic-data/best_synthetic.pt'\n",
    "model = YOLO(model_path)\n",
    "\n",
    "model_path2 = '/kaggle/input/tttttt/best2.pt'\n",
    "model2 = YOLO(model_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bf6aa28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T13:13:15.677713Z",
     "iopub.status.busy": "2025-02-04T13:13:15.677469Z",
     "iopub.status.idle": "2025-02-04T13:13:15.717230Z",
     "shell.execute_reply": "2025-02-04T13:13:15.716396Z"
    },
    "papermill": {
     "duration": 0.045684,
     "end_time": "2025-02-04T13:13:15.718685",
     "exception": false,
     "start_time": "2025-02-04T13:13:15.673001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "runs_path = '/kaggle/input/czii-cryo-et-object-identification/test/static/ExperimentRuns/*'\n",
    "runs = sorted(glob.glob(runs_path))\n",
    "runs = [os.path.basename(run) for run in runs]\n",
    "sp = len(runs)//2\n",
    "runs1 = runs[:sp]\n",
    "runs1[:5]\n",
    "\n",
    "#add by @minfuka\n",
    "runs2 = runs[sp:]\n",
    "runs2[:5]\n",
    "\n",
    "#add by @minfuka\n",
    "assert torch.cuda.device_count() == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9900bc8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T13:13:15.727269Z",
     "iopub.status.busy": "2025-02-04T13:13:15.727048Z",
     "iopub.status.idle": "2025-02-04T13:13:15.731317Z",
     "shell.execute_reply": "2025-02-04T13:13:15.730524Z"
    },
    "papermill": {
     "duration": 0.009812,
     "end_time": "2025-02-04T13:13:15.732503",
     "exception": false,
     "start_time": "2025-02-04T13:13:15.722691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "particle_names = [\n",
    "    'apo-ferritin',\n",
    "    'beta-amylase',\n",
    "    'beta-galactosidase',\n",
    "    'ribosome',\n",
    "    'thyroglobulin',\n",
    "    'virus-like-particle'\n",
    "]\n",
    "\n",
    "particle_to_index = {\n",
    "    'apo-ferritin': 0,\n",
    "    'beta-amylase': 1,\n",
    "    'beta-galactosidase': 2,\n",
    "    'ribosome': 3,\n",
    "    'thyroglobulin': 4,\n",
    "    'virus-like-particle': 5\n",
    "}\n",
    "\n",
    "index_to_particle = {index: name for name, index in particle_to_index.items()}\n",
    "\n",
    "particle_radius = {\n",
    "    'apo-ferritin': 60,\n",
    "    'beta-amylase': 65,\n",
    "    'beta-galactosidase': 90,\n",
    "    'ribosome': 150,\n",
    "    'thyroglobulin': 130,\n",
    "    'virus-like-particle': 135,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "875ea9d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T13:13:15.741437Z",
     "iopub.status.busy": "2025-02-04T13:13:15.741151Z",
     "iopub.status.idle": "2025-02-04T13:13:15.761395Z",
     "shell.execute_reply": "2025-02-04T13:13:15.760607Z"
    },
    "papermill": {
     "duration": 0.026329,
     "end_time": "2025-02-04T13:13:15.762771",
     "exception": false,
     "start_time": "2025-02-04T13:13:15.736442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add by @sesasj\n",
    "class UnionFind:\n",
    "    def __init__(self, size):\n",
    "        self.parent = np.arange(size)\n",
    "        self.rank = np.zeros(size, dtype=int)\n",
    "\n",
    "    def find(self, u):\n",
    "        if self.parent[u] != u:\n",
    "            self.parent[u] = self.find(self.parent[u])  \n",
    "        return self.parent[u]\n",
    "\n",
    "    def union(self, u, v):\n",
    "        u_root = self.find(u)\n",
    "        v_root = self.find(v)\n",
    "        if u_root == v_root:\n",
    "            return\n",
    "            \n",
    "        if self.rank[u_root] < self.rank[v_root]:\n",
    "            self.parent[u_root] = v_root\n",
    "        else:\n",
    "            self.parent[v_root] = u_root\n",
    "            if self.rank[u_root] == self.rank[v_root]:\n",
    "                self.rank[u_root] += 1\n",
    "\n",
    "class PredictionAggregator:\n",
    "    def __init__(self, first_conf=0.2, conf_coef=0.75):\n",
    "        self.first_conf = first_conf\n",
    "        self.conf_coef = conf_coef\n",
    "        self.particle_confs = np.array([0.5, 0.0, 0.2, 0.5, 0.2, 0.5])\n",
    "        \n",
    "    def convert_to_8bit(self, volume):\n",
    "        lower, upper = np.percentile(volume, (0.5, 99.5))\n",
    "        clipped = np.clip(volume, lower, upper)\n",
    "        scaled = ((clipped - lower) / (upper - lower + 1e-12) * 255).astype(np.uint8)\n",
    "        return scaled\n",
    "\n",
    "    def make_predictions(self, run_id, model, device_no):\n",
    "        volume_path = f'/kaggle/input/czii-cryo-et-object-identification/test/static/ExperimentRuns/{run_id}/VoxelSpacing10.000/denoised.zarr'\n",
    "        volume = zarr.open(volume_path, mode='r')[0]\n",
    "        volume_8bit = self.convert_to_8bit(volume)\n",
    "        num_slices = volume_8bit.shape[0]\n",
    "\n",
    "        detections = {\n",
    "            'particle_type': [],\n",
    "            'confidence': [],\n",
    "            'x': [],\n",
    "            'y': [],\n",
    "            'z': []\n",
    "        }\n",
    "\n",
    "        for slice_idx in range(num_slices):\n",
    "            \n",
    "            img = volume_8bit[slice_idx]\n",
    "            input_image = cv2.resize(np.stack([img]*3, axis=-1), (640, 640))\n",
    "\n",
    "            results = model.predict(\n",
    "                input_image,\n",
    "                save=False,\n",
    "                imgsz=640,\n",
    "                conf=self.first_conf,\n",
    "                device=device_no,\n",
    "                batch=1,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            results2 = model2.predict(\n",
    "                input_image,\n",
    "                save=False,\n",
    "                imgsz=640,\n",
    "                conf=self.first_conf,\n",
    "                device=device_no,\n",
    "                batch=1,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            for result in results:\n",
    "                boxes = result.boxes\n",
    "                if boxes is None:\n",
    "                    continue\n",
    "                cls = boxes.cls.cpu().numpy().astype(int)\n",
    "                conf = boxes.conf.cpu().numpy()\n",
    "                xyxy = boxes.xyxy.cpu().numpy()\n",
    "\n",
    "                xc = ((xyxy[:, 0] + xyxy[:, 2]) / 2.0) * 10 * (63/64) # 63/64 because of the resize\n",
    "                yc = ((xyxy[:, 1] + xyxy[:, 3]) / 2.0) * 10 * (63/64)\n",
    "                zc = np.full(xc.shape, slice_idx * 10 + 5)\n",
    "\n",
    "                particle_types = [index_to_particle[c] for c in cls]\n",
    "\n",
    "                detections['particle_type'].extend(particle_types)\n",
    "                detections['confidence'].extend(conf)\n",
    "                detections['x'].extend(xc)\n",
    "                detections['y'].extend(yc)\n",
    "                detections['z'].extend(zc)\n",
    "\n",
    "            for result in results2:\n",
    "                boxes = result.boxes\n",
    "                if boxes is None:\n",
    "                    continue\n",
    "                cls = boxes.cls.cpu().numpy().astype(int)\n",
    "                conf = boxes.conf.cpu().numpy()\n",
    "                xyxy = boxes.xyxy.cpu().numpy()\n",
    "\n",
    "                xc = ((xyxy[:, 0] + xyxy[:, 2]) / 2.0) * 10 * (63/64) # 63/64 because of the resize\n",
    "                yc = ((xyxy[:, 1] + xyxy[:, 3]) / 2.0) * 10 * (63/64)\n",
    "                zc = np.full(xc.shape, slice_idx * 10 + 5)\n",
    "\n",
    "                particle_types = [index_to_particle[c] for c in cls]\n",
    "\n",
    "                detections['particle_type'].extend(particle_types)\n",
    "                detections['confidence'].extend(conf)\n",
    "                detections['x'].extend(xc)\n",
    "                detections['y'].extend(yc)\n",
    "                detections['z'].extend(zc)\n",
    "\n",
    "        if not detections['particle_type']:\n",
    "            return pd.DataFrame()  \n",
    "\n",
    "        particle_types = np.array(detections['particle_type'])\n",
    "        confidences = np.array(detections['confidence'])\n",
    "        xs = np.array(detections['x'])\n",
    "        ys = np.array(detections['y'])\n",
    "        zs = np.array(detections['z'])\n",
    "\n",
    "        aggregated_data = []\n",
    "\n",
    "        for idx, particle in enumerate(particle_names):\n",
    "            if particle == 'beta-amylase':\n",
    "                continue \n",
    "\n",
    "            mask = (particle_types == particle)\n",
    "            if not np.any(mask):\n",
    "                continue  \n",
    "                \n",
    "            particle_confidences = confidences[mask]\n",
    "            particle_xs = xs[mask]\n",
    "            particle_ys = ys[mask]\n",
    "            particle_zs = zs[mask]\n",
    "            # -------------modified by @sersasj ------------------------\n",
    "            coords = np.vstack((particle_xs, particle_ys, particle_zs)).T\n",
    "\n",
    "           \n",
    "            z_distance = 30 # How many slices can you \"jump\" to aggregate predictions 10 = 1, 20 = 2...\n",
    "            xy_distance = 20 # xy_tol_p2 in original code by ITK8191\n",
    "            \n",
    "            max_distance = math.sqrt(z_distance**2 + xy_distance**2)\n",
    "            tree = cKDTree(coords)            \n",
    "            pairs = tree.query_pairs(r=max_distance, p=2)\n",
    "\n",
    "            \n",
    "            uf = UnionFind(len(coords))\n",
    "            \n",
    "            coords_xy = coords[:, :2]\n",
    "            coords_z = coords[:, 2]\n",
    "            for u, v in pairs:\n",
    "                z_diff = abs(coords_z[u] - coords_z[v])\n",
    "                if z_diff > z_distance:\n",
    "                    continue  \n",
    "\n",
    "                xy_diff = np.linalg.norm(coords_xy[u] - coords_xy[v])\n",
    "                if xy_diff > xy_distance:\n",
    "                    continue  \n",
    "\n",
    "                uf.union(u, v)\n",
    "\n",
    "            roots = np.array([uf.find(i) for i in range(len(coords))])\n",
    "            unique_roots, inverse_indices, counts = np.unique(roots, return_inverse=True, return_counts=True)\n",
    "            conf_sums = np.bincount(inverse_indices, weights=particle_confidences)\n",
    "            \n",
    "            aggregated_confidences = conf_sums / (counts ** self.conf_coef)\n",
    "            cluster_per_particle = [4*2,1*2,2*2,9*2,4*2,8*2]\n",
    "            valid_clusters = (counts >= cluster_per_particle[idx]) & (aggregated_confidences > self.particle_confs[idx])\n",
    "\n",
    "            if not np.any(valid_clusters):\n",
    "                continue  \n",
    "\n",
    "            cluster_ids = unique_roots[valid_clusters]\n",
    "\n",
    "            centers_x = np.bincount(inverse_indices, weights=particle_xs) / counts\n",
    "            centers_y = np.bincount(inverse_indices, weights=particle_ys) / counts\n",
    "            centers_z = np.bincount(inverse_indices, weights=particle_zs) / counts\n",
    "\n",
    "            centers_x = centers_x[valid_clusters]\n",
    "            centers_y = centers_y[valid_clusters]\n",
    "            centers_z = centers_z[valid_clusters]\n",
    "\n",
    "            aggregated_df = pd.DataFrame({\n",
    "                'experiment': [run_id] * len(centers_x),\n",
    "                'particle_type': [particle] * len(centers_x),\n",
    "                'x': centers_x,\n",
    "                'y': centers_y,\n",
    "                'z': centers_z\n",
    "            })\n",
    "\n",
    "            aggregated_data.append(aggregated_df)\n",
    "\n",
    "        if aggregated_data:\n",
    "            return pd.concat(aggregated_data, axis=0)\n",
    "        else:\n",
    "            return pd.DataFrame()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90d81aba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T13:13:15.771210Z",
     "iopub.status.busy": "2025-02-04T13:13:15.770972Z",
     "iopub.status.idle": "2025-02-04T13:13:53.660975Z",
     "shell.execute_reply": "2025-02-04T13:13:53.659847Z"
    },
    "papermill": {
     "duration": 37.896167,
     "end_time": "2025-02-04T13:13:53.662792",
     "exception": false,
     "start_time": "2025-02-04T13:13:15.766625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:19<00:00, 19.06s/it]\n",
      "100%|██████████| 2/2 [00:36<00:00, 18.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated total prediction time for 500 runs: 6313.5096 seconds\n"
     ]
    }
   ],
   "source": [
    "# instance main class\n",
    "aggregator = PredictionAggregator(first_conf=0.22,  conf_coef=0.39)#Update\n",
    "aggregated_results = []\n",
    "#add by @minfuka\n",
    "from concurrent.futures import ProcessPoolExecutor #add by @minfuka\n",
    "\n",
    "#add by @minfuka\n",
    "def inference(runs, model, device_no):\n",
    "    subs = []\n",
    "    for r in tqdm(runs, total=len(runs)):\n",
    "        df = aggregator.make_predictions(r, model, device_no)\n",
    "        subs.append(df)\n",
    "    \n",
    "    return subs\n",
    "start_time = time.time()\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=2) as executor:\n",
    "    results = list(executor.map(inference, (runs1, runs2), (model, model), (\"0\", \"1\")))\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "estimated_total_time = (end_time - start_time) / len(runs) * 500  \n",
    "print(f'estimated total prediction time for 500 runs: {estimated_total_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff13df94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T13:13:53.678229Z",
     "iopub.status.busy": "2025-02-04T13:13:53.677946Z",
     "iopub.status.idle": "2025-02-04T13:13:53.685141Z",
     "shell.execute_reply": "2025-02-04T13:13:53.684526Z"
    },
    "papermill": {
     "duration": 0.015269,
     "end_time": "2025-02-04T13:13:53.686410",
     "exception": false,
     "start_time": "2025-02-04T13:13:53.671141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#change by @minfuka\n",
    "submission0 = pd.concat(results[0])\n",
    "submission1 = pd.concat(results[1])\n",
    "submission_ = pd.concat([submission0, submission1]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc69f6d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T13:13:53.695834Z",
     "iopub.status.busy": "2025-02-04T13:13:53.695546Z",
     "iopub.status.idle": "2025-02-04T13:13:53.702849Z",
     "shell.execute_reply": "2025-02-04T13:13:53.702214Z"
    },
    "papermill": {
     "duration": 0.013354,
     "end_time": "2025-02-04T13:13:53.704081",
     "exception": false,
     "start_time": "2025-02-04T13:13:53.690727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_.insert(0, 'id', range(len(submission_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c622f4",
   "metadata": {
    "papermill": {
     "duration": 0.003975,
     "end_time": "2025-02-04T13:13:53.712403",
     "exception": false,
     "start_time": "2025-02-04T13:13:53.708428",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **《《《 Unet3D(Monai) 》》》**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3883e4b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T13:13:53.721784Z",
     "iopub.status.busy": "2025-02-04T13:13:53.721575Z",
     "iopub.status.idle": "2025-02-04T13:15:02.428693Z",
     "shell.execute_reply": "2025-02-04T13:15:02.427920Z"
    },
    "papermill": {
     "duration": 68.713802,
     "end_time": "2025-02-04T13:15:02.430358",
     "exception": false,
     "start_time": "2025-02-04T13:13:53.716556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-fb75f2711b5d>:155: DeprecationWarning: config_type not found in config file, defaulting to filesystem\n",
      "  root = copick.from_file(copick_test_config_path)\n",
      "Loading dataset: 100%|██████████| 98/98 [00:00<00:00, 144.03it/s]\n",
      "100%|██████████| 98/98 [00:11<00:00,  8.24it/s]\n",
      "Loading dataset: 100%|██████████| 98/98 [00:00<00:00, 181.24it/s]\n",
      "100%|██████████| 98/98 [00:11<00:00,  8.51it/s]\n",
      "Loading dataset: 100%|██████████| 98/98 [00:00<00:00, 148.78it/s]\n",
      "100%|██████████| 98/98 [00:11<00:00,  8.30it/s]\n"
     ]
    }
   ],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        spatial_dims: int = 3,\n",
    "        in_channels: int = 1,\n",
    "        out_channels: int = 7,\n",
    "        channels: Union[Tuple[int, ...], List[int]] = (48, 64, 80, 80),\n",
    "        strides: Union[Tuple[int, ...], List[int]] = (2, 2, 1),\n",
    "        num_res_units: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = UNet(\n",
    "            spatial_dims=self.hparams.spatial_dims,\n",
    "            in_channels=self.hparams.in_channels,\n",
    "            out_channels=self.hparams.out_channels,\n",
    "            channels=self.hparams.channels,\n",
    "            strides=self.hparams.strides,\n",
    "            num_res_units=self.hparams.num_res_units,\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "channels = (48, 64, 80, 80)\n",
    "strides_pattern = (2, 2, 1)\n",
    "num_res_units = 1\n",
    "def extract_3d_patches_minimal_overlap(arrays: List[np.ndarray], patch_size: int) -> Tuple[List[np.ndarray], List[Tuple[int, int, int]]]:\n",
    "    if not arrays or not isinstance(arrays, list):\n",
    "        raise ValueError(\"Input must be a non-empty list of arrays\")\n",
    "    \n",
    "    # Verify all arrays have the same shape\n",
    "    shape = arrays[0].shape\n",
    "    if not all(arr.shape == shape for arr in arrays):\n",
    "        raise ValueError(\"All input arrays must have the same shape\")\n",
    "    \n",
    "    if patch_size > min(shape):\n",
    "        raise ValueError(f\"patch_size ({patch_size}) must be smaller than smallest dimension {min(shape)}\")\n",
    "    \n",
    "    m, n, l = shape\n",
    "    patches = []\n",
    "    coordinates = []\n",
    "    \n",
    "    # Calculate starting positions for each dimension\n",
    "    x_starts = calculate_patch_starts(m, patch_size)\n",
    "    y_starts = calculate_patch_starts(n, patch_size)\n",
    "    z_starts = calculate_patch_starts(l, patch_size)\n",
    "    \n",
    "    # Extract patches from each array\n",
    "    for arr in arrays:\n",
    "        for x in x_starts:\n",
    "            for y in y_starts:\n",
    "                for z in z_starts:\n",
    "                    patch = arr[\n",
    "                        x:x + patch_size,\n",
    "                        y:y + patch_size,\n",
    "                        z:z + patch_size\n",
    "                    ]\n",
    "                    patches.append(patch)\n",
    "                    coordinates.append((x, y, z))\n",
    "    \n",
    "    return patches, coordinates\n",
    "def reconstruct_array(patches: List[np.ndarray], \n",
    "                     coordinates: List[Tuple[int, int, int]], \n",
    "                     original_shape: Tuple[int, int, int]) -> np.ndarray:\n",
    "    reconstructed = np.zeros(original_shape, dtype=np.int64)  # To track overlapping regions\n",
    "    \n",
    "    patch_size = patches[0].shape[0]\n",
    "    \n",
    "    for patch, (x, y, z) in zip(patches, coordinates):\n",
    "        reconstructed[\n",
    "            x:x + patch_size,\n",
    "            y:y + patch_size,\n",
    "            z:z + patch_size\n",
    "        ] = patch\n",
    "        \n",
    "    \n",
    "    return reconstructed\n",
    "def calculate_patch_starts(dimension_size: int, patch_size: int) -> List[int]:\n",
    "    if dimension_size <= patch_size:\n",
    "        return [0]\n",
    "        \n",
    "    # Calculate number of patches needed\n",
    "    n_patches = np.ceil(dimension_size / patch_size)\n",
    "    \n",
    "    if n_patches == 1:\n",
    "        return [0]\n",
    "    \n",
    "    # Calculate overlap\n",
    "    total_overlap = (n_patches * patch_size - dimension_size) / (n_patches - 1)\n",
    "    \n",
    "    # Generate starting positions\n",
    "    positions = []\n",
    "    for i in range(int(n_patches)):\n",
    "        pos = int(i * (patch_size - total_overlap))\n",
    "        if pos + patch_size > dimension_size:\n",
    "            pos = dimension_size - patch_size\n",
    "        if pos not in positions:  # Avoid duplicates\n",
    "            positions.append(pos)\n",
    "    \n",
    "    return positions\n",
    "import pandas as pd\n",
    "\n",
    "def dict_to_df(coord_dict, experiment_name):\n",
    "    # Create lists to store data\n",
    "    all_coords = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Process each label and its coordinates\n",
    "    for label, coords in coord_dict.items():\n",
    "        all_coords.append(coords)\n",
    "        all_labels.extend([label] * len(coords))\n",
    "    \n",
    "    # Concatenate all coordinates\n",
    "    all_coords = np.vstack(all_coords)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'experiment': experiment_name,\n",
    "        'particle_type': all_labels,\n",
    "        'x': all_coords[:, 0],\n",
    "        'y': all_coords[:, 1],\n",
    "        'z': all_coords[:, 2]\n",
    "    })\n",
    "\n",
    "    \n",
    "    return df\n",
    "from typing import List, Tuple, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "from monai.data import DataLoader, Dataset, CacheDataset, decollate_batch\n",
    "from monai.transforms import (\n",
    "    Compose, \n",
    "    EnsureChannelFirstd, \n",
    "    Orientationd,  \n",
    "    AsDiscrete,  \n",
    "    RandFlipd, \n",
    "    RandRotate90d, \n",
    "    NormalizeIntensityd,\n",
    "    RandCropByLabelClassesd,\n",
    ")\n",
    "TRAIN_DATA_DIR = \"/kaggle/input/create-numpy-dataset-exp-name\"\n",
    "import json\n",
    "copick_config_path = TRAIN_DATA_DIR + \"/copick.config\"\n",
    "\n",
    "with open(copick_config_path) as f:\n",
    "    copick_config = json.load(f)\n",
    "\n",
    "copick_config['static_root'] = '/kaggle/input/czii-cryo-et-object-identification/test/static'\n",
    "\n",
    "copick_test_config_path = 'copick_test.config'\n",
    "\n",
    "with open(copick_test_config_path, 'w') as outfile:\n",
    "    json.dump(copick_config, outfile)\n",
    "import copick\n",
    "\n",
    "root = copick.from_file(copick_test_config_path)\n",
    "\n",
    "copick_user_name = \"copickUtils\"\n",
    "copick_segmentation_name = \"paintedPicks\"\n",
    "voxel_size = 10\n",
    "tomo_type = \"denoised\"\n",
    "inference_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\"], channel_dim=\"no_channel\"),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    Orientationd(keys=[\"image\"], axcodes=\"RAS\")\n",
    "])\n",
    "import cc3d\n",
    "\n",
    "id_to_name = {1: \"apo-ferritin\", \n",
    "              2: \"beta-amylase\",\n",
    "              3: \"beta-galactosidase\", \n",
    "              4: \"ribosome\", \n",
    "              5: \"thyroglobulin\", \n",
    "              6: \"virus-like-particle\"}\n",
    "BLOB_THRESHOLD = 200\n",
    "CERTAINTY_THRESHOLD = 0.05\n",
    "\n",
    "classes = [1, 2, 3, 4, 5, 6]\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cc3d\n",
    "from monai.data import CacheDataset\n",
    "from monai.transforms import Compose, EnsureType\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from monai.networks.nets import UNet\n",
    "from monai.losses import TverskyLoss\n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "def load_models(model_paths):\n",
    "    models = []\n",
    "    for model_path in model_paths:\n",
    "        channels = (48, 64, 80, 80)\n",
    "        strides_pattern = (2, 2, 1)       \n",
    "        num_res_units = 1\n",
    "        learning_rate = 1e-3\n",
    "        num_epochs = 100\n",
    "        model = Model(channels=channels, strides=strides_pattern, num_res_units=num_res_units)\n",
    "        \n",
    "        weights =torch.load(model_path)['state_dict']\n",
    "        model.load_state_dict(weights)\n",
    "        model.to('cuda')\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "\n",
    "model_paths = [\n",
    "    '/kaggle/input/tttttt/1checkpoint_epoch191-val_metric0.77.ckpt',\n",
    "]\n",
    "\n",
    "\n",
    "models = load_models(model_paths)\n",
    "def ensemble_prediction_tta(models, input_tensor, threshold=0.5):\n",
    "    augmented_tensors = [\n",
    "        input_tensor,  \n",
    "        torch.flip(input_tensor.clone(), dims=[2]),  \n",
    "        torch.flip(input_tensor.clone(), dims=[3]),  \n",
    "        torch.flip(input_tensor.clone(), dims=[4])  \n",
    "    ]\n",
    "    batch = torch.cat(augmented_tensors, dim=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for model in models:\n",
    "            model_output = model(batch)\n",
    "            model_output[1] = torch.flip(model_output[1], dims=[1])\n",
    "            model_output[2] = torch.flip(model_output[2], dims=[2])\n",
    "            model_output[3] = torch.flip(model_output[3], dims=[3])\n",
    "            probs = torch.softmax(model_output, dim=1)\n",
    "            \n",
    "    avg_probs = torch.mean(probs, dim=0)\n",
    "    thresh_probs = avg_probs > threshold\n",
    "    _, max_classes = thresh_probs.max(dim=0)\n",
    "    return max_classes\n",
    "sub=[]\n",
    "for model in models:\n",
    "    with torch.no_grad():\n",
    "        location_df = []\n",
    "        for run in root.runs:\n",
    "            tomo = run.get_voxel_spacing(10)\n",
    "            tomo = tomo.get_tomogram(tomo_type).numpy()\n",
    "            tomo_patches, coordinates = extract_3d_patches_minimal_overlap([tomo], 96)\n",
    "            tomo_patched_data = [{\"image\": img} for img in tomo_patches]\n",
    "            tomo_ds = CacheDataset(data=tomo_patched_data, transform=inference_transforms, cache_rate=1.0)\n",
    "            pred_masks = []\n",
    "            for i in tqdm(range(len(tomo_ds))):\n",
    "                input_tensor = tomo_ds[i]['image'].unsqueeze(0).to(\"cuda\")\n",
    "                max_classes = ensemble_prediction_tta(models, input_tensor, threshold=CERTAINTY_THRESHOLD)\n",
    "                pred_masks.append(max_classes.cpu().numpy())\n",
    "            reconstructed_mask = reconstruct_array(pred_masks, coordinates, tomo.shape)\n",
    "            location = {}\n",
    "            for c in classes:\n",
    "                cc = cc3d.connected_components(reconstructed_mask == c)\n",
    "                stats = cc3d.statistics(cc)\n",
    "                zyx = stats['centroids'][1:] * 10.012444  # 转换单位\n",
    "                zyx_large = get_zyx_large4(zyx, stats, c)\n",
    "                #zyx_large = zyx[stats['voxel_counts'][1:] > BLOB_THRESHOLD]\n",
    "                xyz = np.ascontiguousarray(zyx_large[:, ::-1])\n",
    "                location[id_to_name[c]] = xyz\n",
    "            df = dict_to_df(location, run.name)\n",
    "            location_df.append(df)\n",
    "        location_df = pd.concat(location_df)\n",
    "        location_df.insert(loc=0, column='id', value=np.arange(len(location_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9649fd51",
   "metadata": {
    "papermill": {
     "duration": 0.017721,
     "end_time": "2025-02-04T13:15:02.466828",
     "exception": false,
     "start_time": "2025-02-04T13:15:02.449107",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **《《《 Finaly Blend 》》》**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98e861fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T13:15:02.504593Z",
     "iopub.status.busy": "2025-02-04T13:15:02.503755Z",
     "iopub.status.idle": "2025-02-04T13:15:04.250830Z",
     "shell.execute_reply": "2025-02-04T13:15:04.249710Z"
    },
    "papermill": {
     "duration": 1.768287,
     "end_time": "2025-02-04T13:15:04.252953",
     "exception": false,
     "start_time": "2025-02-04T13:15:02.484666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "df = pd.concat([submission_,location_df], ignore_index=True)\n",
    "\n",
    "particle_names = ['apo-ferritin', 'beta-amylase', 'beta-galactosidase', 'ribosome', 'thyroglobulin', 'virus-like-particle']\n",
    "particle_radius = {\n",
    "    'apo-ferritin': 60,\n",
    "    'beta-amylase': 65,\n",
    "    'beta-galactosidase': 90,\n",
    "    'ribosome': 150,\n",
    "    'thyroglobulin': 130,\n",
    "    'virus-like-particle': 135,\n",
    "}\n",
    "\n",
    "final = []\n",
    "for pidx, p in enumerate(particle_names):\n",
    "    pdf = df[df['particle_type'] == p].reset_index(drop=True)\n",
    "    p_rad = particle_radius[p]\n",
    "    \n",
    "    grouped = pdf.groupby(['experiment'])\n",
    "    \n",
    "    for exp, group in grouped:\n",
    "        group = group.reset_index(drop=True)\n",
    "        \n",
    "        coords = group[['x', 'y', 'z']].values\n",
    "        db = DBSCAN(eps=p_rad, min_samples=2, metric='euclidean').fit(coords)\n",
    "        labels = db.labels_\n",
    "        \n",
    "        group['cluster'] = labels\n",
    "        \n",
    "        for cluster_id in np.unique(labels):\n",
    "            if cluster_id == -1:\n",
    "                continue\n",
    "            \n",
    "            cluster_points = group[group['cluster'] == cluster_id]\n",
    "            \n",
    "            avg_x = cluster_points['x'].mean()\n",
    "            avg_y = cluster_points['y'].mean()\n",
    "            avg_z = cluster_points['z'].mean()\n",
    "            \n",
    "            group.loc[group['cluster'] == cluster_id, ['x', 'y', 'z']] = avg_x, avg_y, avg_z\n",
    "            group = group.drop_duplicates(subset=['x', 'y', 'z'])\n",
    "        final.append(group)\n",
    "\n",
    "df_save = pd.concat(final, ignore_index=True)\n",
    "df_save = df_save.drop(columns=['cluster'])\n",
    "df_save = df_save.sort_values(by=['experiment', 'particle_type']).reset_index(drop=True)\n",
    "df_save['id'] = np.arange(0, len(df_save))\n",
    "df_save.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6bb2e80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T13:15:04.293755Z",
     "iopub.status.busy": "2025-02-04T13:15:04.293471Z",
     "iopub.status.idle": "2025-02-04T13:15:04.309715Z",
     "shell.execute_reply": "2025-02-04T13:15:04.308846Z"
    },
    "papermill": {
     "duration": 0.036107,
     "end_time": "2025-02-04T13:15:04.310966",
     "exception": false,
     "start_time": "2025-02-04T13:15:04.274859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>experiment</th>\n",
       "      <th>particle_type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>TS_5_4</td>\n",
       "      <td>apo-ferritin</td>\n",
       "      <td>5468.542285</td>\n",
       "      <td>1520.825789</td>\n",
       "      <td>82.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>TS_5_4</td>\n",
       "      <td>apo-ferritin</td>\n",
       "      <td>5868.350122</td>\n",
       "      <td>5136.230737</td>\n",
       "      <td>85.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>TS_5_4</td>\n",
       "      <td>apo-ferritin</td>\n",
       "      <td>5713.249296</td>\n",
       "      <td>4999.417072</td>\n",
       "      <td>120.164943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>TS_5_4</td>\n",
       "      <td>apo-ferritin</td>\n",
       "      <td>5743.285753</td>\n",
       "      <td>5105.443576</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>TS_5_4</td>\n",
       "      <td>apo-ferritin</td>\n",
       "      <td>5291.644482</td>\n",
       "      <td>4169.832178</td>\n",
       "      <td>135.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>833</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>5771.520833</td>\n",
       "      <td>4593.417345</td>\n",
       "      <td>975.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>834</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>4513.058317</td>\n",
       "      <td>5148.046244</td>\n",
       "      <td>1173.124232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>835</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>1453.606485</td>\n",
       "      <td>4806.548102</td>\n",
       "      <td>1120.180998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>836</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>2500.982038</td>\n",
       "      <td>1321.115637</td>\n",
       "      <td>520.415221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>837</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>817.867318</td>\n",
       "      <td>1819.512769</td>\n",
       "      <td>1081.191829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>838 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id experiment        particle_type            x            y  \\\n",
       "0      0     TS_5_4         apo-ferritin  5468.542285  1520.825789   \n",
       "1      1     TS_5_4         apo-ferritin  5868.350122  5136.230737   \n",
       "2      2     TS_5_4         apo-ferritin  5713.249296  4999.417072   \n",
       "3      3     TS_5_4         apo-ferritin  5743.285753  5105.443576   \n",
       "4      4     TS_5_4         apo-ferritin  5291.644482  4169.832178   \n",
       "..   ...        ...                  ...          ...          ...   \n",
       "833  833     TS_6_4  virus-like-particle  5771.520833  4593.417345   \n",
       "834  834     TS_6_4  virus-like-particle  4513.058317  5148.046244   \n",
       "835  835     TS_6_4  virus-like-particle  1453.606485  4806.548102   \n",
       "836  836     TS_6_4  virus-like-particle  2500.982038  1321.115637   \n",
       "837  837     TS_6_4  virus-like-particle   817.867318  1819.512769   \n",
       "\n",
       "               z  \n",
       "0      82.333333  \n",
       "1      85.000000  \n",
       "2     120.164943  \n",
       "3      90.000000  \n",
       "4     135.000000  \n",
       "..           ...  \n",
       "833   975.000000  \n",
       "834  1173.124232  \n",
       "835  1120.180998  \n",
       "836   520.415221  \n",
       "837  1081.191829  \n",
       "\n",
       "[838 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a217af",
   "metadata": {
    "papermill": {
     "duration": 0.018033,
     "end_time": "2025-02-04T13:15:04.347643",
     "exception": false,
     "start_time": "2025-02-04T13:15:04.329610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10033515,
     "sourceId": 84969,
     "sourceType": "competition"
    },
    {
     "datasetId": 6052780,
     "sourceId": 9862305,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6040935,
     "sourceId": 9867543,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6465904,
     "sourceId": 10445850,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6484063,
     "sourceId": 10471985,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6548541,
     "sourceId": 10581622,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6533363,
     "sourceId": 10589134,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6566482,
     "sourceId": 10607486,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6577603,
     "sourceId": 10623321,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6585830,
     "sourceId": 10636911,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 206640467,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 211097053,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 199.861944,
   "end_time": "2025-02-04T13:15:07.500951",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-04T13:11:47.639007",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
